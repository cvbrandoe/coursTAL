{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp2ZBaTcnGmR"
      },
      "source": [
        "# Introduction au NEL - approche supervisée\n",
        "\n",
        "Utilisation du module Entity Linked de la bibliothèque Spacy. Exemple par Sofie Van Landeghem (Spacy) adapté au Spacy v3 et traduit au français."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1q2O7N8anGmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae86fde1-cfca-4678-e7c3-d7b3c521684c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy==3.0.6\n",
            "  Downloading spacy-3.0.6.tar.gz (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Collecting spacy-lookups-data\n",
            "  Downloading spacy_lookups_data-1.0.5-py2.py3-none-any.whl (98.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy-lookups-data) (67.7.2)\n",
            "Installing collected packages: spacy-lookups-data\n",
            "Successfully installed spacy-lookups-data-1.0.5\n",
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy==3.0.6\n",
        "!pip install spacy-lookups-data\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18cjsI2KnGmU"
      },
      "source": [
        "Application d'un modèle anglais pré-entraîné sur un échantillon de texte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j_h-9nmJnGmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0718f7ab-7165-4e73-992d-492dc20e5c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entity 'Emerson' with label 'PERSON'\n",
            "Named Entity 'Wimbledon' with label 'EVENT'\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "text = \"Tennis champion Emerson was expected to win Wimbledon.\"\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print(f\"Named Entity '{ent.text}' with label '{ent.label_}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_mRh5GdnGmV"
      },
      "source": [
        "Nous constatons que cette phrase contient une personne appelée \"Emerson\" et un événement appelé \"Wimbledon\".\n",
        "\n",
        "Malheureusement, il peut y avoir de nombreuses personnes dans le monde qui s'appellent \"Emerson\", et ce résultat ne nous dit toujours pas de laquelle il s'agit exactement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mISImTHQnGmV"
      },
      "source": [
        "Dans ce cas précis, la phrase nous donne des indices importants : Emerson est manifestement un joueur de tennis professionnel.\n",
        "\n",
        "En effectuant une recherche sur l'internet, nous pouvons établir que cette phrase parle très probablement de Roy Emerson, un joueur de tennis australien. Nous pouvons à présent associer à cette entité de la phrase son identifiant unique WikiData.\n",
        "Ses identifiants uniques commencent toujours par un Q, et \"Roy Emerson\" a l'identifiant Q312545 : https://www.wikidata.org/wiki/Q312545\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppOUsz5anGmV"
      },
      "source": [
        "Pour mettre en œuvre un pipeline Entity Linker, nous avons besoin de trois étapes différentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjEPeu7fnGmW"
      },
      "source": [
        "# Creation de la base de connaissance (KB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULUleo40nGmW"
      },
      "source": [
        "La première étape consiste à créer une KB contenant les identifiants uniques des entités qui nous intéressent.\n",
        "\n",
        "La KB stocke des vecteurs d'entités pré-entraînés. Ces vecteurs sont des versions condensées des descriptions des entités. Des embeddings plus importants permettent de capturer plus d'informations, mais nécessitent également plus de stockage.\n",
        "\n",
        "Dans ce tutoriel, nous en créerons une KB très simple avec seulement 3 entrées. Nous chargeons les données à partir d'un fichier CSV prédéfini.\n",
        "\n",
        "Pour cela, il faut copier le fichier entites.csv dans l'espace de travail Google Colaboratory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0eahB3_dnGmW"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "\n",
        "def load_entities():\n",
        "    entities_loc = Path.cwd().parent / \"content\" / \"entities.csv\"  # distributed alongside this notebook\n",
        "\n",
        "    names = dict()\n",
        "    descriptions = dict()\n",
        "    with entities_loc.open(\"r\", encoding=\"utf8\") as csvfile:\n",
        "        csvreader = csv.reader(csvfile, delimiter=\",\")\n",
        "        for row in csvreader:\n",
        "            qid = row[0]\n",
        "            name = row[1]\n",
        "            desc = row[2]\n",
        "            names[qid] = name\n",
        "            descriptions[qid] = desc\n",
        "    return names, descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oEdBc3_tnGmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2849a571-c65a-4443-a23f-304aa5016aaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q312545, name=Roy Stanley Emerson, desc=Australian tennis player\n",
            "Q48226, name=Ralph Waldo Emerson, desc=American philosopher, essayist, and poet\n",
            "Q215952, name=Emerson Ferreira da Rosa, desc=Brazilian footballer\n"
          ]
        }
      ],
      "source": [
        "name_dict, desc_dict = load_entities()\n",
        "for QID in name_dict.keys():\n",
        "    print(f\"{QID}, name={name_dict[QID]}, desc={desc_dict[QID]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6vERx2ZnGmW"
      },
      "source": [
        "Nous avons ici 3 entrées, de 3 personnes différentes appelées Emerson. Un joueur de tennis australien, un écrivain américain et un footballeur brésilien. Nous utiliserons ces informations pour créer notre base de connaissances. Nous devons définir une dimensionnalité fixe pour les vecteurs d'entités, qui sera 300-D dans notre cas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Pj43D3KdnGmW"
      },
      "outputs": [],
      "source": [
        "from spacy.kb import InMemoryLookupKB\n",
        "vocab = nlp.vocab\n",
        "kb = InMemoryLookupKB(vocab=vocab, entity_vector_length=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "948rurBBnGmX"
      },
      "source": [
        "Pour ajouter chaque entrée à la KB, nous encodons sa description en utilisant les vecteurs de mots de notre modèle `nlp`. L'attribut `vector` d'un document est la moyenne de ses vecteurs de mots. Nous devons également fournir une fréquence, qui est un compte brut du nombre de fois qu'une certaine entité apparaît dans un corpus annoté. Dans ce tutoriel, nous n'utilisons pas ces fréquences, donc nous les fixons à une valeur arbitraire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cK_rkUwPnGmX"
      },
      "outputs": [],
      "source": [
        "for qid, desc in desc_dict.items():\n",
        "    desc_doc = nlp(desc)\n",
        "    desc_enc = desc_doc.vector\n",
        "    kb.add_entity(entity=qid, entity_vector=desc_enc, freq=342)   # 342 is an arbitrary value here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu0iy4ymnGmX"
      },
      "source": [
        "Nous voulons maintenant spécifier des alias ou des synonymes. Nous commençons par ajouter les noms complets. Ici, nous sommes sûrs à 100 % qu'ils se résolvent à leur QID correspondant, puisqu'il n'y a pas d'ambiguïté."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vCAcw4UvnGmX"
      },
      "outputs": [],
      "source": [
        "for qid, name in name_dict.items():\n",
        "    kb.add_alias(alias=name, entities=[qid], probabilities=[1])   # 100% prior probability P(entity|alias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCee8WpRnGmX"
      },
      "source": [
        "Nous voulons également ajouter le pseudonyme \"Emerson\". Nous supposerons que chacun de nos trois Emerson est également célèbre et nous fixerons donc des probabilités égales pour chaque entité."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9ID7bGWpnGmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67f45ca1-64d2-496e-a502-f964361ab8b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4831166512461469197"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "qids = name_dict.keys()\n",
        "probs = [0.3 for qid in qids]\n",
        "kb.add_alias(alias=\"Emerson\", entities=qids, probabilities=probs)  # sum([probs]) should be <= 1 !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7y4lf8EnGmX"
      },
      "source": [
        "Ce sera donc notre base de connaissances. Nous pouvons vérifier les entités et les alias qu'elle contient :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7-Dd7r8inGmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d043b8d5-9726-424b-b305-ad7ebd37c17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities in the KB: ['Q215952', 'Q312545', 'Q48226']\n",
            "Aliases in the KB: ['Roy Stanley Emerson', 'Emerson Ferreira da Rosa', 'Ralph Waldo Emerson', 'Emerson']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Entities in the KB: {kb.get_entity_strings()}\")\n",
        "print(f\"Aliases in the KB: {kb.get_alias_strings()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khxuuFlbnGmX"
      },
      "source": [
        "Nous pouvons également imprimer les candidats générés pour le nom complet de Roy Emerson, ainsi que pour la mention \"Emerson\" ou pour toute autre mention aléatoire, comme \"Charles\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "G11C3MDVnGmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf991ab5-8576-4ad9-f2af-1c1e0b3cab3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Candidates for 'Roy Stanley Emerson': ['Q312545']\n",
            "Candidates for 'Emerson': ['Q312545', 'Q48226', 'Q215952']\n",
            "Candidates for 'Charles': []\n"
          ]
        }
      ],
      "source": [
        "print(f\"Candidates for 'Roy Stanley Emerson': {[c.entity_ for c in kb.get_alias_candidates('Roy Stanley Emerson')]}\")\n",
        "print(f\"Candidates for 'Emerson': {[c.entity_ for c in kb.get_alias_candidates('Emerson')]}\")\n",
        "print(f\"Candidates for 'Charles': {[c.entity_ for c in kb.get_alias_candidates('Sofie')]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOqCV-5knGmY"
      },
      "source": [
        "Nous remarquons que l'interrogation de la KB avec l'alias \"Emerson\" nous donne 3 candidats, mais si nous l'interrogeons avec un terme inconnu, nous obtenons une liste vide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AAmP--dnGmY"
      },
      "source": [
        "Nous pouvons sauvegarder la base de connaissances en appelant la fonction `to_disk` avec un emplacement de sortie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8EV0sVG0nGmY"
      },
      "outputs": [],
      "source": [
        "# change the directory and file names to whatever you like\n",
        "import os\n",
        "output_dir = Path.cwd().parent / \"content\" / \"my_output\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "kb.to_disk(output_dir / \"my_kb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWhGhl4BnGmY"
      },
      "source": [
        "Nous pouvons stocker l'objet `nlp` dans un fichier en appelant aussi `to_disk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eAYnh3NjnGmY"
      },
      "outputs": [],
      "source": [
        "nlp.to_disk(output_dir / \"my_nlp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d77hSpv5nGmY"
      },
      "source": [
        "# Creation du jeu d'entrainement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WN1BlU4nGmY"
      },
      "source": [
        "Nous pouvons stocker l'objet `nlp` dans un fichier en appelant `to_disk` également. Maintenant, nous devons créer des données annotées (ici en JSONL) pour entraîner un algorithme de liaison d'entités."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gO7ery-EnGmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32a2bda-a008-4a36-8bfe-fd107b9e555e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"text\":\"Interestingly, Emerson is one of only five tennis players all-time to win multiple slam sets in two disciplines, only matched by Frank Sedgman, Margaret Court, Martina Navratilova and Serena Williams.\",\"_input_hash\":2024197919,\"_task_hash\":-1926469210,\"spans\":[{\"start\":15,\"end\":22,\"text\":\"Emerson\",\"rank\":0,\"label\":\"ORG\",\"score\":1,\"source\":\"en_core_web_lg\",\"input_hash\":2024197919}],\"meta\":{\"score\":1},\"options\":[{\"id\":\"Q48226\",\"html\":\"<a href='https://www.wikidata.org/wiki/Q48226'>Q48226: American philosopher, essayist, and poet</a>\"},{\"id\":\"Q215952\",\"html\":\"<a href='https://www.wikidata.org/wiki/Q215952'>Q215952: Brazilian footballer</a>\"},{\"id\":\"Q312545\",\"html\":\"<a href='https://www.wikidata.org/wiki/Q312545'>Q312545: Australian tennis player</a>\"},{\"id\":\"NIL_otherLink\",\"text\":\"Link not in options\"},{\"id\":\"NIL_ambiguous\",\"text\":\"Need more context\"}],\"_session_id\":null,\"_view_id\":\"choice\",\"accept\":[\"Q312545\"],\"answer\":\"accept\"}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "json_loc = Path.cwd().parent / \"content\" / \"emerson_annotated_text.jsonl\" # distributed alongside this notebook\n",
        "with json_loc.open(\"r\", encoding=\"utf8\") as jsonfile:\n",
        "    line = jsonfile.readline()\n",
        "    print(line)   # print just the first line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YDiQmB_nGmZ"
      },
      "source": [
        "Nous voyons que le texte complet de la phrase originale est stocké, ainsi que de nombreux détails sur la tâche d'annotation. La partie la plus importante est stockée avec la clé `accept` à la fin : c'est la valeur de notre annotation manuelle. Pour cette phrase spécifique et cette mention spécifique, l'option avec la clé `Q312545` a été sélectionnée manuellement. C'est sur cette information que nous allons entraîner notre éditeur de liens d'entités."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPEiSzIhnGmZ"
      },
      "source": [
        "# Entrainer l'Entity Linker de Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSt8EAx3nGmZ"
      },
      "source": [
        "Pour alimenter notre Entity Linker en données d'entraînement, nous formatons nos données sous la forme d'un tuple structuré. La première partie est le texte brut, et la seconde partie est un dictionnaire d'annotations. Ce dictionnaire définit les entités nommées que nous voulons lier (\"entités\"), ainsi que les liens de référence (\"liens\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8kmEzLEwnGmZ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "dataset = []\n",
        "json_loc = Path.cwd().parent / \"content\" / \"emerson_annotated_text.jsonl\"\n",
        "with json_loc.open(\"r\", encoding=\"utf8\") as jsonfile:\n",
        "    for line in jsonfile:\n",
        "        example = json.loads(line)\n",
        "        text = example[\"text\"]\n",
        "        if example[\"answer\"] == \"accept\":\n",
        "            QID = example[\"accept\"][0]\n",
        "            offset = (example[\"spans\"][0][\"start\"], example[\"spans\"][0][\"end\"])\n",
        "            entity_label = example[\"spans\"][0][\"label\"]\n",
        "            entities = [(offset[0], offset[1], entity_label)]\n",
        "            links_dict = {QID: 1.0}\n",
        "        dataset.append((text, {\"links\": {offset: links_dict}, \"entities\": entities}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czdrz2I3nGmZ"
      },
      "source": [
        "Pour vérifier si la conversion est correcte, il suffit d'imprimer le premier échantillon de notre ensemble de données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dqisTbHGnGma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e4a201f-36ba-428c-e8fd-7e78e9d4a499"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Interestingly, Emerson is one of only five tennis players all-time to win multiple slam sets in two disciplines, only matched by Frank Sedgman, Margaret Court, Martina Navratilova and Serena Williams.',\n",
              " {'links': {(15, 22): {'Q312545': 1.0}}, 'entities': [(15, 22, 'ORG')]})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLNI6WHZnGma"
      },
      "source": [
        "Nous pouvons également vérifier certaines statistiques dans cet ensemble de données. Combien de cas de chaque QID avons-nous annotés ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ka95qmrgnGma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056777f1-6b41-445a-e304-6a53136ae5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'Q312545': 10, 'Q48226': 10, 'Q215952': 10})\n"
          ]
        }
      ],
      "source": [
        "gold_ids = []\n",
        "for text, annot in dataset:\n",
        "    for span, links_dict in annot[\"links\"].items():\n",
        "        for link, value in links_dict.items():\n",
        "            if value:\n",
        "                gold_ids.append(link)\n",
        "\n",
        "from collections import Counter\n",
        "print(Counter(gold_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTvhZXHjnGma"
      },
      "source": [
        "Nous avons obtenu exactement 10 phrases annotées pour chacun de nos Emerson. Parmi ces phrases, nous allons maintenant mettre de côté 6 cas dans un ensemble de test séparé."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tpJN5VuwnGma"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "train_dataset = []\n",
        "test_dataset = []\n",
        "for QID in qids:\n",
        "    indices = [i for i, j in enumerate(gold_ids) if j == QID]\n",
        "    train_dataset.extend(dataset[index] for index in indices[0:8])  # first 8 in training\n",
        "    test_dataset.extend(dataset[index] for index in indices[8:10])  # last 2 in test\n",
        "\n",
        "random.shuffle(train_dataset)\n",
        "random.shuffle(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXXBmZ49nGma"
      },
      "source": [
        "Avec nos ensembles de données correctement configurés, nous allons maintenant créer des objets `Exemple` pour alimenter le processus de formation. Essentiellement, il contient un document avec des prédictions (`predicted`) et un autre avec des annotations gold-standard (`reference`). Au cours de l'apprentissage, le pipeline comparera ses prédictions au gold-standard et mettra à jour les poids du réseau neuronal en conséquence.\n",
        "\n",
        "Pour le NEL, l'algorithme a besoin d'accéder à des phrases du jeu de validation, car les algorithmes utilisent le contexte de la phrase pour effectuer la désambiguïsation. Vous pouvez soit fournir des annotations `sent_starts` du jeu de validation, soit exécuter un composant tel que `parser` ou `sentencizer` sur vos documents de référence :\n",
        "\n",
        "For entity linking, the algorithm needs access to gold-standard sentences, because the algorithms use the context from the sentence to perform the disambiguation. You can either provide gold-standard `sent_starts` annotations, or run a component such as the `parser` or `sentencizer` on your reference documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8PB4FzbwnGma"
      },
      "outputs": [],
      "source": [
        "from spacy.training import Example\n",
        "\n",
        "TRAIN_EXAMPLES = []\n",
        "if \"sentencizer\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(\"sentencizer\")\n",
        "sentencizer = nlp.get_pipe(\"sentencizer\")\n",
        "for text, annotation in train_dataset:\n",
        "    example = Example.from_dict(nlp.make_doc(text), annotation)\n",
        "    example.reference = sentencizer(example.reference)\n",
        "    TRAIN_EXAMPLES.append(example)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajnzHYhLnGma"
      },
      "source": [
        "Ensuite, nous allons créer un nouveau composant Entity Linking et l'ajouter au pipeline.\n",
        "\n",
        "Nous devons également nous assurer que le composant `entity_linker` est correctement initialisé. Pour ce faire, nous avons besoin d'une fonction `get_examples` qui retourne des données d'entraînement, ainsi qu'un argument `kb_loader`. Il s'agit d'une fonction \"appelable\" qui crée la `KnowledgeBase` à partir d'une certaine instance de `Vocab`. Ici, nous allons charger notre KB depuis le disque, en utilisant la fonction intégrée [`spacy.KBFromFile.v1`](https://spacy.io/api/architectures#KBFromFile), qui est définie dans `spacy.ml.models`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "P99C1pYQnGmb"
      },
      "outputs": [],
      "source": [
        "from spacy.ml.models import load_kb\n",
        "\n",
        "entity_linker = nlp.add_pipe(\"entity_linker\", config={\"incl_prior\": False}, last=True)\n",
        "entity_linker.initialize(get_examples=lambda: TRAIN_EXAMPLES, kb_loader=load_kb(output_dir / \"my_kb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znAthK4CnGmb"
      },
      "source": [
        "Ensuite, nous exécuterons la boucle d'apprentissage proprement dite pour le nouveau composant, en veillant à n'entraîner que l'entity linker et non les autres composants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1ye2hnUBnGmb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbcb1ce3-6b31-45e6-9148-e45beb728451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Losses {'entity_linker': 5.335945904254913}\n",
            "50 Losses {'entity_linker': 0.6727053821086884}\n",
            "100 Losses {'entity_linker': 0.7969951430956523}\n",
            "150 Losses {'entity_linker': 0.9766097664833069}\n",
            "200 Losses {'entity_linker': 0.6734015047550201}\n",
            "250 Losses {'entity_linker': 0.47187170386314387}\n",
            "300 Losses {'entity_linker': 0.8068462312221527}\n",
            "350 Losses {'entity_linker': 0.22556909918785095}\n",
            "400 Losses {'entity_linker': 0.9543093045552571}\n",
            "450 Losses {'entity_linker': 0.9101343353589375}\n",
            "499 Losses {'entity_linker': 0.48602598905563354}\n"
          ]
        }
      ],
      "source": [
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "with nlp.select_pipes(enable=[\"entity_linker\"]):   # train only the entity_linker\n",
        "    optimizer = nlp.resume_training()\n",
        "    for itn in range(500):   # 500 iterations takes about a minute to train\n",
        "        random.shuffle(TRAIN_EXAMPLES)\n",
        "        batches = minibatch(TRAIN_EXAMPLES, size=compounding(4.0, 32.0, 1.001))  # increasing batch sizes\n",
        "        losses = {}\n",
        "        for batch in batches:\n",
        "            nlp.update(\n",
        "                batch,\n",
        "                drop=0.2,      # prevent overfitting\n",
        "                losses=losses,\n",
        "                sgd=optimizer,\n",
        "            )\n",
        "        if itn % 50 == 0:\n",
        "            print(itn, \"Losses\", losses)   # print the training loss\n",
        "print(itn, \"Losses\", losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY9c86ZZnGmb"
      },
      "source": [
        "La valeur Loss (fonction de perte) de la dernière boucle d'apprentissage est assez faible, ce qui est bon signe. Mais pour vraiment vérifier si notre modèle se généralise bien, nous devons le tester sur des données inédites.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTLMZmCMnGmb"
      },
      "source": [
        "# Tester l'Entity Linker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD_APRrPnGmb"
      },
      "source": [
        "Appliquons-le d'abord à notre phrase originale. Pour chaque entité, nous imprimons le texte et l'étiquette comme précédemment, mais aussi le QID désambiguïsé tel que prédit par notre entity linker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4x6g8eamnGmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef91902-6087-4d49-d024-e60796662afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emerson PERSON Q215952\n",
            "Wimbledon EVENT NIL\n"
          ]
        }
      ],
      "source": [
        "text = \"Tennis champion Emerson was expected to win Wimbledon.\"\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_, ent.kb_id_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA3fsJiGnGmc"
      },
      "source": [
        "Nous voyons qu'Emerson est désambiguïsé en Q312545, qui est l'identifiant correct du joueur de tennis. Notez également que l'entité \"Wimbledon\" reçoit l'annotation `NIL`, qui est essentiellement une valeur de remplacement, montrant que le composant NEL n'a pas pu trouver d'identifiant pertinent pour cette entité. Cela s'explique par le fait que notre base de connaissances et le composant Entity Linking n'ont été entrainés que sur des exemples \"Emerson\", et sont donc assez limités."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tObyjMHynGmc"
      },
      "source": [
        "Voyons ce que le modèle prédit pour les 6 phrases de notre ensemble de données de test, qui n'ont jamais été vues pendant l'entrainement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "85YO2JXtnGmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f584cc-91af-4a8b-fa6b-29b645bc95b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emerson scored his second international goal on 31 March 1999, in a friendly match against Japan in Tokyo, which Brazil won 2-0.\n",
            "Gold annotation: {'links': {(0, 7): {'Q215952': 1.0}}, 'entities': [(0, 7, 'ORG')]}\n",
            "Prediction: Emerson, ORG, Q215952\n",
            "\n",
            "Emerson was inducted into the International Tennis Hall of Fame in 1982 and the Sport Australia Hall of Fame in 1986.\n",
            "Gold annotation: {'links': {(0, 7): {'Q312545': 1.0}}, 'entities': [(0, 7, 'ORG')]}\n",
            "Prediction: Emerson, ORG, Q215952\n",
            "\n",
            "Carlyle in particular was a strong influence on him; Emerson would later serve as an unofficial literary agent in the United States for Carlyle, and in March 1835, he tried to persuade Carlyle to come to America to lecture.\n",
            "Gold annotation: {'links': {(53, 60): {'Q48226': 1.0}}, 'entities': [(53, 60, 'ORG')]}\n",
            "Prediction: Emerson, ORG, Q215952\n",
            "\n",
            "Emerson made his Brazil debut on 10 September 1997, in a home friendly match against Ecuador, in Salvador, Bahia, also scoring a goal in the match, as Brazil went on to win 4-2.\n",
            "Gold annotation: {'links': {(0, 7): {'Q215952': 1.0}}, 'entities': [(0, 7, 'ORG')]}\n",
            "Prediction: Emerson, ORG, Q215952\n",
            "\n",
            "Emerson's first Wimbledon singles title came in 1964, with a final victory over Fred Stolle.\n",
            "Gold annotation: {'links': {(0, 7): {'Q312545': 1.0}}, 'entities': [(0, 7, 'ORG')]}\n",
            "Prediction: Emerson, ORG, Q215952\n",
            "\n",
            "In 1841 Emerson published Essays, his second book, which included the famous essay \"Self-Reliance\".\n",
            "Gold annotation: {'links': {(8, 15): {'Q48226': 1.0}}, 'entities': [(8, 15, 'PERSON')]}\n",
            "Prediction: Emerson, ORG, Q215952\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for text, true_annot in test_dataset:\n",
        "    print(text)\n",
        "    print(f\"Gold annotation: {true_annot}\")\n",
        "    doc = nlp(text)  # to make this more efficient, you can use nlp.pipe() just once for all the texts\n",
        "    for ent in doc.ents:\n",
        "        if ent.text == \"Emerson\":\n",
        "            print(f\"Prediction: {ent.text}, {ent.label_}, {ent.kb_id_}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8SaxyYynGmc"
      },
      "source": [
        "Ces résultats peuvent varier légèrement d'un cycle à l'autre, mais en général, le pipeline EL obtient 5 prédictions correctes sur 6 (83 % de précision). Une supposition aléatoire n'aurait permis d'obtenir que 33 %."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}